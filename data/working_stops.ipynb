{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ff48f9-0a86-491a-86c8-848a9be334dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "\n",
    "# Base directory containing GTFS feeds\n",
    "base_dir = \"gtfs\"\n",
    "\n",
    "# Get all subdirectories in base directory\n",
    "sub_dirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "# Lists to store processed stops and routes\n",
    "stops_features = []\n",
    "routes_features = []\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    feed_name = os.path.basename(sub_dir)\n",
    "    print(f\"Processing feed: {feed_name}\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Load routes.txt (route metadata)\n",
    "    # ---------------------\n",
    "    routes_file = os.path.join(sub_dir, \"routes.txt\")\n",
    "    if os.path.exists(routes_file):\n",
    "        routes_df = pd.read_csv(routes_file, dtype={\"route_id\": str, \"route_short_name\": str, \"route_long_name\": str, \n",
    "                                                     \"route_desc\": str, \"route_type\": str, \"route_color\": str})\n",
    "        routes_df[\"route_color\"] = routes_df[\"route_color\"].fillna(\"000000\").apply(lambda x: f\"#{x.zfill(6)}\")  # Ensure HEX format\n",
    "    else:\n",
    "        print(f\"Missing routes.txt in {sub_dir}\")\n",
    "        routes_df = pd.DataFrame(columns=[\"route_id\", \"route_short_name\", \"route_long_name\", \"route_desc\", \"route_type\", \"route_color\"])\n",
    "\n",
    "    # ---------------------\n",
    "    # Load trips.txt (trip_id → route_id mapping)\n",
    "    # ---------------------\n",
    "    trips_file = os.path.join(sub_dir, \"trips.txt\")\n",
    "    if os.path.exists(trips_file):\n",
    "        trips_df = pd.read_csv(trips_file, usecols=[\"trip_id\", \"route_id\", \"shape_id\"], dtype={\"trip_id\": str, \"route_id\": str, \"shape_id\": str}).dropna()\n",
    "        trips_df = trips_df.drop_duplicates(subset=[\"shape_id\", \"route_id\"])\n",
    "    else:\n",
    "        print(f\"Missing trips.txt in {sub_dir}\")\n",
    "        trips_df = pd.DataFrame(columns=[\"trip_id\", \"route_id\", \"shape_id\"])\n",
    "\n",
    "    # Merge trips with routes to get full route metadata\n",
    "    trips_routes_df = trips_df.merge(routes_df, on=\"route_id\", how=\"left\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Process Routes (shapes.txt)\n",
    "    # ---------------------\n",
    "    shapes_file = os.path.join(sub_dir, \"shapes.txt\")\n",
    "    if os.path.exists(shapes_file):\n",
    "        shapes_df = pd.read_csv(shapes_file, usecols=[\"shape_id\", \"shape_pt_lat\", \"shape_pt_lon\", \"shape_pt_sequence\"], dtype={\"shape_id\": str})\n",
    "\n",
    "        # Merge with trips_routes_df to add route metadata to each shape\n",
    "        shapes_merged_df = shapes_df.merge(trips_routes_df, on=\"shape_id\", how=\"left\")\n",
    "\n",
    "        # Aggregate to create LineStrings\n",
    "        shapes_grouped = shapes_merged_df.groupby(\"shape_id\").agg({\n",
    "            \"shape_pt_lon\": list,\n",
    "            \"shape_pt_lat\": list,\n",
    "            \"route_id\": \"first\",\n",
    "            \"route_short_name\": \"first\",\n",
    "            \"route_long_name\": \"first\",\n",
    "            \"route_desc\": \"first\",\n",
    "            \"route_type\": \"first\",\n",
    "            \"route_color\": \"first\"\n",
    "        }).reset_index()\n",
    "\n",
    "        # Convert grouped points into LineStrings\n",
    "        shapes_grouped[\"geometry\"] = shapes_grouped.apply(lambda row: LineString(zip(row[\"shape_pt_lon\"], row[\"shape_pt_lat\"])), axis=1)\n",
    "        \n",
    "        # Add feed name\n",
    "        shapes_grouped[\"feed\"] = feed_name\n",
    "\n",
    "        # Convert to dictionary format\n",
    "        routes_features.extend(shapes_grouped.to_dict(orient=\"records\"))\n",
    "    else:\n",
    "        print(f\"Missing shapes.txt in {sub_dir}\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Process Stops (stops.txt)\n",
    "    # ---------------------\n",
    "    stops_file = os.path.join(sub_dir, \"stops.txt\")\n",
    "    if os.path.exists(stops_file):\n",
    "        stops_df = pd.read_csv(stops_file, dtype={\"stop_id\": str, \"stop_name\": str, \"stop_lat\": float, \"stop_lon\": float})\n",
    "    else:\n",
    "        print(f\"Missing stops.txt in {sub_dir}\")\n",
    "        continue  # Skip if no stops data\n",
    "\n",
    "    # Load stop_times.txt (stop_id → trip_id mapping)\n",
    "    stop_times_file = os.path.join(sub_dir, \"stop_times.txt\")\n",
    "    if os.path.exists(stop_times_file):\n",
    "        stop_times_df = pd.read_csv(stop_times_file, usecols=[\"stop_id\", \"trip_id\"], dtype={\"stop_id\": str, \"trip_id\": str}).dropna()\n",
    "    else:\n",
    "        print(f\"Missing stop_times.txt in {sub_dir}\")\n",
    "        continue  # Skip if missing\n",
    "\n",
    "    # Merge stop_times with trips to get route_id per stop\n",
    "    stop_routes_df = stop_times_df.merge(trips_df, on=\"trip_id\", how=\"left\").drop(columns=[\"trip_id\"])\n",
    "\n",
    "    # Aggregate all routes per stop\n",
    "    stop_routes_grouped = stop_routes_df.groupby(\"stop_id\")[\"route_id\"].unique().reset_index()\n",
    "\n",
    "    # Convert route_id list to a comma-separated string\n",
    "    stop_routes_grouped[\"routes\"] = stop_routes_grouped[\"route_id\"].apply(lambda x: \",\".join(sorted(map(str, x))) if pd.notnull(x).all() else \"\")\n",
    "\n",
    "    stop_routes_grouped = stop_routes_grouped.drop(columns=[\"route_id\"])\n",
    "\n",
    "    # Merge stops with route data\n",
    "    stops_merged_df = stops_df.merge(stop_routes_grouped, on=\"stop_id\", how=\"left\").fillna(\"\")\n",
    "\n",
    "    # Assign route colors (using the first listed route)\n",
    "    def get_primary_route_color(route_list):\n",
    "        if not route_list:\n",
    "            return \"#000000\"\n",
    "        first_route = route_list.split(\",\")[0] if \",\" in route_list else route_list\n",
    "        color = routes_df[routes_df[\"route_id\"] == first_route][\"route_color\"].values\n",
    "        return color[0] if len(color) > 0 else \"#000000\"\n",
    "\n",
    "    stops_merged_df[\"route_color\"] = stops_merged_df[\"routes\"].apply(get_primary_route_color)\n",
    "\n",
    "    # Convert to GeoJSON format\n",
    "    stops_merged_df[\"geometry\"] = stops_merged_df.apply(lambda row: Point(row[\"stop_lon\"], row[\"stop_lat\"]), axis=1)\n",
    "    stops_merged_df[\"feed\"] = feed_name\n",
    "\n",
    "    # Convert to dictionary format\n",
    "    stops_features.extend(stops_merged_df.to_dict(orient=\"records\"))\n",
    "\n",
    "# ---------------------\n",
    "# Export GeoJSON files\n",
    "# ---------------------\n",
    "if routes_features:\n",
    "    routes_gdf = gpd.GeoDataFrame(routes_features, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "    routes_output = \"routes.geojson\"\n",
    "    routes_gdf.to_file(routes_output, driver=\"GeoJSON\")\n",
    "    print(f\"Created {routes_output}\")\n",
    "else:\n",
    "    print(\"No route data found.\")\n",
    "\n",
    "if stops_features:\n",
    "    stops_gdf = gpd.GeoDataFrame(stops_features, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "    stops_output = \"stops.geojson\"\n",
    "    stops_gdf.to_file(stops_output, driver=\"GeoJSON\")\n",
    "    print(f\"Created {stops_output}\")\n",
    "else:\n",
    "    print(\"No stop data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b9f2f7-9051-4c43-bce9-295017edbef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Base directory containing GTFS feeds\n",
    "base_dir = \"gtfs\"\n",
    "\n",
    "# Get all subdirectories in base directory\n",
    "sub_dirs = [os.path.join(base_dir, d) for d in os.listdir(base_dir) if os.path.isdir(os.path.join(base_dir, d))]\n",
    "\n",
    "# List to store processed stops\n",
    "stops_features = []\n",
    "\n",
    "for sub_dir in sub_dirs:\n",
    "    feed_name = os.path.basename(sub_dir)\n",
    "    print(f\"Processing feed: {feed_name}\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Load stops.txt (stop locations)\n",
    "    # ---------------------\n",
    "    stops_file = os.path.join(sub_dir, \"stops.txt\")\n",
    "    if os.path.exists(stops_file):\n",
    "        stops_df = pd.read_csv(stops_file, dtype={\"stop_id\": str, \"stop_name\": str, \"stop_lat\": float, \"stop_lon\": float})\n",
    "    else:\n",
    "        print(f\"Missing stops.txt in {sub_dir}\")\n",
    "        continue  # Skip if no stops data\n",
    "\n",
    "    # ---------------------\n",
    "    # Load stop_times.txt (stop_id → trip_id mapping)\n",
    "    # ---------------------\n",
    "    stop_times_file = os.path.join(sub_dir, \"stop_times.txt\")\n",
    "    if os.path.exists(stop_times_file):\n",
    "        stop_times_df = pd.read_csv(stop_times_file, usecols=[\"stop_id\", \"trip_id\"], dtype={\"stop_id\": str, \"trip_id\": str}).dropna()\n",
    "    else:\n",
    "        print(f\"Missing stop_times.txt in {sub_dir}\")\n",
    "        continue  # Skip if missing\n",
    "\n",
    "    # ---------------------\n",
    "    # Load trips.txt (trip_id → route_id mapping)\n",
    "    # ---------------------\n",
    "    trips_file = os.path.join(sub_dir, \"trips.txt\")\n",
    "    if os.path.exists(trips_file):\n",
    "        trips_df = pd.read_csv(trips_file, usecols=[\"trip_id\", \"route_id\"], dtype={\"trip_id\": str, \"route_id\": str}).dropna()\n",
    "    else:\n",
    "        print(f\"Missing trips.txt in {sub_dir}\")\n",
    "        continue  # Skip if missing\n",
    "\n",
    "    # ---------------------\n",
    "    # Merge stop_times with trips to get route_id per stop\n",
    "    # ---------------------\n",
    "    stop_routes_df = stop_times_df.merge(trips_df, on=\"trip_id\", how=\"left\").drop(columns=[\"trip_id\"])\n",
    "\n",
    "    # ---------------------\n",
    "    # Aggregate all routes per stop\n",
    "    # ---------------------\n",
    "    stop_routes_grouped = stop_routes_df.groupby(\"stop_id\")[\"route_id\"].unique().reset_index()\n",
    "\n",
    "    # Convert route_id list to a comma-separated string\n",
    "    stop_routes_grouped[\"routes\"] = stop_routes_grouped[\"route_id\"].apply(\n",
    "        lambda x: \",\".join(sorted(map(str, x))) if pd.notnull(x).all() else \"\"\n",
    "    )\n",
    "\n",
    "    stop_routes_grouped = stop_routes_grouped.drop(columns=[\"route_id\"])\n",
    "\n",
    "    # ---------------------\n",
    "    # Merge stops with route data\n",
    "    # ---------------------\n",
    "    stops_merged_df = stops_df.merge(stop_routes_grouped, on=\"stop_id\", how=\"left\").fillna(\"\")\n",
    "\n",
    "    # ---------------------\n",
    "    # Load routes.txt (to get colors)\n",
    "    # ---------------------\n",
    "    routes_file = os.path.join(sub_dir, \"routes.txt\")\n",
    "    if os.path.exists(routes_file):\n",
    "        routes_df = pd.read_csv(routes_file, usecols=[\"route_id\", \"route_color\"], dtype={\"route_id\": str, \"route_color\": str})\n",
    "        routes_df[\"route_color\"] = routes_df[\"route_color\"].fillna(\"000000\").apply(lambda x: f\"#{x.zfill(6)}\")  # Ensure HEX color format\n",
    "    else:\n",
    "        print(f\"Missing routes.txt in {sub_dir}\")\n",
    "        routes_df = pd.DataFrame(columns=[\"route_id\", \"route_color\"])\n",
    "\n",
    "    # ---------------------\n",
    "    # Assign route colors (using the first listed route)\n",
    "    # ---------------------\n",
    "    def get_primary_route_color(route_list):\n",
    "        if not route_list:\n",
    "            return \"#000000\"\n",
    "        first_route = route_list.split(\",\")[0] if \",\" in route_list else route_list\n",
    "        color = routes_df[routes_df[\"route_id\"] == first_route][\"route_color\"].values\n",
    "        return color[0] if len(color) > 0 else \"#000000\"\n",
    "\n",
    "    stops_merged_df[\"route_color\"] = stops_merged_df[\"routes\"].apply(get_primary_route_color)\n",
    "\n",
    "    # ---------------------\n",
    "    # Convert to GeoJSON format\n",
    "    # ---------------------\n",
    "    stops_merged_df[\"geometry\"] = stops_merged_df.apply(lambda row: Point(row[\"stop_lon\"], row[\"stop_lat\"]), axis=1)\n",
    "    stops_merged_df[\"feed\"] = feed_name\n",
    "\n",
    "    # Convert to dictionary format for GeoJSON output\n",
    "    stops_features.extend(stops_merged_df.to_dict(orient=\"records\"))\n",
    "\n",
    "# ---------------------\n",
    "# Export stops.geojson\n",
    "# ---------------------\n",
    "if stops_features:\n",
    "    stops_gdf = gpd.GeoDataFrame(stops_features, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "    stops_output = \"output/stops.geojson\"\n",
    "    stops_gdf.to_file(stops_output, driver=\"GeoJSON\")\n",
    "    print(f\"Created {stops_output}\")\n",
    "else:\n",
    "    print(\"No stop data found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2ce1fa-eefe-4eff-ae1e-4e32e95dca34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.tail()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "onebusmap",
   "language": "python",
   "name": "onebusmap"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
